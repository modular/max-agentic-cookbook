[project]
authors = [{ name = "Modular", email = "hello@modular.com" }]
name = "max-serve-anythingllm"
requires-python = ">= 3.11"
version = "0.1.0"
dependencies = []

[build-system]
build-backend = "hatchling.build"
requires = ["hatchling"]

[tool.hatch.build.targets.wheel]
packages = ["."]

[tool.pixi.project]
channels = [
    "conda-forge",
    "https://conda.modular.com/max-nightly",
]
platforms = ["linux-64", "linux-aarch64", "osx-arm64"]

[tool.pixi.pypi-dependencies]
max_serve_anythingllm = { path = ".", editable = true }

[tool.pixi.activation.env]
MAX_SECRETS_LOCATION = ".env"
MAX_CONTEXT_LENGTH = "16384"
MAX_BATCH_SIZE = "1"
MAX_SERVE_PORT = "3002"
UI_PORT = "3001"
UI_STORAGE_LOCATION = "./data"
UI_CONTAINER_NAME = "anythingllm-max"

[tool.pixi.tasks]
app = "python main.py llm ui --pre setup --post clean"
setup = "python setup.py"
llm = "max serve  --max-length=$MAX_CONTEXT_LENGTH --max-batch-size=$MAX_BATCH_SIZE --model-path=modularai/Llama-3.1-8B-Instruct-GGUF"
ui = "docker run -p $UI_PORT:3001 --name $UI_CONTAINER_NAME --cap-add SYS_ADMIN -v $UI_STORAGE_LOCATION:/app/storage -v $UI_STORAGE_LOCATION/.env:/app/server/.env -e STORAGE_DIR=\"/app/storage\" docker.io/mintplexlabs/anythingllm"
clean = "pkill -f \"max serve\" || true && lsof -ti:$MAX_SERVE_PORT,$UI_PORT | xargs -r kill -9 2>/dev/null || true && docker rm -f $UI_CONTAINER_NAME 2>/dev/null || true"
tests = "python test.py"

[tool.pixi.dependencies]
modular = ">=25.5.0.dev2025070905"
honcho = ">=2.0.0"
tomli = ">=2.2.1"
python-dotenv = ">=1.0.1"
click = ">=8.1.8"
