[project]
authors = [{ name = "Modular", email = "hello@modular.com" }]
name = "max-serve-anythingllm"
requires-python = ">= 3.11"
version = "0.1.0"
dependencies = []

[build-system]
build-backend = "hatchling.build"
requires = ["hatchling"]

[tool.hatch.build.targets.wheel]
packages = ["."]

[tool.pixi.project]
channels = [
    "https://conda.modular.com/max-nightly",
    "https://conda.modular.com/max",
    "https://repo.prefix.dev/modular-community",
    "conda-forge",
]
platforms = ["linux-64", "linux-aarch64", "osx-arm64"]

[tool.pixi.pypi-dependencies]
max_serve_anythingllm = { path = ".", editable = true }

[tool.pixi.activation.env]
MAX_LLM_PORT = "3002"
UI_PORT = "3001"
UI_STORAGE_LOCATION = "./data"
UI_CONTAINER_NAME = "anythingllm-max"

[tool.pixi.tasks]
app = "python main.py"
ui = "docker run -p $UI_PORT:3001 --name $UI_CONTAINER_NAME --cap-add SYS_ADMIN -v $UI_STORAGE_LOCATION:/app/server/storage -v $UI_STORAGE_LOCATION/.env:/app/server/.env -e STORAGE_DIR=\"/app/server/storage\" mintplexlabs/anythingllm"
llm = "export MAX_SERVE_PORT=$MAX_LLM_PORT && max-pipelines serve  --max-length=16384 --max-batch-size=1 --model-path=deepseek-ai/DeepSeek-R1-Distill-Llama-8B --weight-path=lmstudio-community/DeepSeek-R1-Distill-Llama-8B-GGUF/DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf"
clean = "pkill -f \"max-pipelines serve\" || true && lsof -ti:$MAX_LLM_PORT,$UI_PORT | xargs -r kill -9 2>/dev/null || true && docker rm -f $UI_CONTAINER_NAME 2>/dev/null || true"

[tool.pixi.dependencies]
max-pipelines = ">=25.2.0.dev2025022405,<26"
honcho = ">=2.0.0,<3"
tomli = ">=2.2.1,<3"
python-dotenv = ">=1.0.1,<2"
