version: 1.0
long_title: "Run inference with MAX using a simple Python script"
short_title: "Offline inference with MAX"
author: "Bill Welense"
author_image: "author/billw.jpg"
author_url: "https://www.linkedin.com/in/welense/"
github_repo: "https://github.com/modular/max-recipes/tree/main/max-serve-openai-embeddings"
date: "13-02-2025"
difficulty: "beginner"
tags:
  - max
  - llama

tasks:
  - magic run app
