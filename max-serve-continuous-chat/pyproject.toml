[project]
authors = [{ name = "Modular Inc", email = "hello@modular.com" }]
description = "Continuous Chat App with MAX Serve and lama3"
name = "max-serve-continuous-chat"
requires-python = ">= 3.9,<3.13"
version = "0.0.0"
dependencies = [
    "gradio>=5.8.0,<6",
    "fastapi>=0.115.6,<0.116",
    "requests>=2.32.3,<3",
    "openai>=1.57.3,<2",
    "tenacity>=9.0.0,<10",
    "transformers>=4.47.0,<5",
    "click>=8.1.7,<9",
]

[build-system]
build-backend = "hatchling.build"
requires = ["hatchling"]

[tool.hatch.build.targets.wheel]
packages = ["."]

[tool.pixi.project]
channels = [
    "conda-forge",
    "https://conda.modular.com/max-nightly",
    "https://conda.modular.com/max",
    "https://repo.prefix.dev/modular-community",
]
platforms = ["linux-64", "osx-arm64", "linux-aarch64"]

[tool.pixi.pypi-dependencies]
max-serve-continuous-chat = { path = ".", editable = true }

[tool.pixi.tasks]
app = "bash run.sh app"
stop = "bash run.sh stop"
clean = "bash run.sh clean"

[tool.pixi.dependencies]
docker-compose = ">=2.32.4,<3"
bash = ">=5.2.21,<6"
