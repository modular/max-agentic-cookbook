MODEL_NAME="meta-llama/Llama-3.2-1B-Instruct"

# Don't have a compatible GPU?
# https://docs.modular.com/max/faq#gpu-requirements
# Uncomment this line for quantized CPU weights:
# MODEL_WEIGHTS="bartowski/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct-Q4_K_M.gguf"

API_HOST=0.0.0.0
API_PORT=8080

MAX_SERVE_HOST=0.0.0.0
MAX_SERVE_PORT=8000
MAX_SERVE_API_PATH="/v1"
MAX_SERVE_HEALTH_PATH="/v1/health"

MCP_HOST=0.0.0.0
MCP_PORT=8001
MCP_API_PATH="/mcp"
MCP_HEALTH_PATH="/health"