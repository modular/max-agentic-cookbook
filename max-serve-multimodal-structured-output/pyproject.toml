[project]
authors = [{ name = "Modular Inc", email = "hello@modular.com" }]
dependencies = ["pydantic", "openai>=1.61.1,<2", "tenacity>=9.0.0,<10", "requests>=2.32.3,<3"]
name = "max-serve-multimodal-structured-output"
requires-python = ">=3.10,<3.13"
version = "0.0.0"

[system-requirements]
cuda = "12.5"

[build-system]
build-backend = "hatchling.build"
requires = ["hatchling"]

[tool.hatch.build.targets.wheel]
packages = ["."]

[tool.pixi.project]
channels = ["conda-forge", "https://conda.modular.com/max-nightly"]
platforms = ["linux-64", "linux-aarch64"]

[tool.pixi.pypi-dependencies]
max_serve_multimodal_structured_output = { path = ".", editable = true }

[tool.pixi.tasks]
server = "MAX_SERVE_PORT=8010 max serve --model-path meta-llama/Llama-3.2-11B-Vision-Instruct --max-length=1000 --max-batch-size=1 --enable-structured-output"
main = "python main.py"
tests = "echo 'test passed'"

[tool.pixi.dependencies]
honcho = ">=2.0.0,<3"
