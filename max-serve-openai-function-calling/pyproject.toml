[project]
authors = [{ name = "Modular ", email = "hello@modular.com" }]
description = "Add a short description here"
name = "max-serve-openai-function-calling"
requires-python = ">= 3.11,<3.13"
version = "0.0.0"
dependencies = [
    "openai>=1.60.2,<2",
    "fastapi>=0.115.7,<0.116",
    "pydantic>=2.10.6,<3",
    "requests>=2.32.3,<3",
    "python-dotenv>=1.0.1,<2",
    "uvicorn>=0.34.0,<0.35", "httpx>=0.28.1,<0.29",
]

[build-system]
build-backend = "hatchling.build"
requires = ["hatchling"]

[tool.hatch.build.targets.wheel]
packages = ["."]

[tool.pixi.project]
channels = [
    "conda-forge",
    "https://conda.modular.com/max-nightly",
]
platforms = ["linux-64", "osx-arm64", "linux-aarch64"]

[tool.pixi.pypi-dependencies]
max_serve_openai_function_calling = { path = ".", editable = true }

[tool.pixi.tasks]
server = "MAX_SERVE_PORT=8077 max-pipelines serve --model-path modularai/Llama-3.1-8B-Instruct-GGUF --max-length=2048"
single_function_call = "magic run python single_function_call.py"
multi_function_calls = "magic run python multi_function_calls.py"
app = "magic run python app.py"
tests = "echo 'test passed'"

[tool.pixi.dependencies]
bash = ">=5.2.21,<6"
